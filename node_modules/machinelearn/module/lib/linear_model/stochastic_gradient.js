import * as tf from '@tensorflow/tfjs';
import { cloneDeep, range } from 'lodash';
import * as Random from 'random-js';
import { validateFitInputs, validateMatrix2D } from '../ops';
export var TypeLoss;
(function (TypeLoss) {
    TypeLoss["L1"] = "L1";
    TypeLoss["L2"] = "L2";
    TypeLoss["L1L2"] = "L1L2";
})(TypeLoss || (TypeLoss = {}));
/**
 * Ordinary base class for SGD classier or regressor
 * @ignore
 */
export class BaseSGD {
    /**
     * @param preprocess - preprocess methodology can be either minmax or null. Default is minmax.
     * @param learning_rate - Used to limit the amount each coefficient is corrected each time it is updated.
     * @param epochs - Number of iterations.
     * @param clone - To clone the passed in dataset.
     */
    constructor({ learning_rate = 0.0001, epochs = 10000, clone = true, random_state = null, loss = TypeLoss.L2, reg_factor = null } = {
        learning_rate: 0.0001,
        epochs: 10000,
        clone: true,
        random_state: null,
        loss: TypeLoss.L2,
        reg_factor: null
    }) {
        this.clone = true;
        this.weights = null;
        this.learningRate = learning_rate;
        this.epochs = epochs;
        this.clone = clone;
        this.randomState = random_state;
        this.loss = loss;
        this.regFactor = reg_factor;
        // Setting a loss function according to the input option
        if (this.loss === TypeLoss.L1 && this.regFactor) {
            this.loss = tf.regularizers.l1({
                l1: this.regFactor.l1
            });
        }
        else if (this.loss === TypeLoss.L1L2 && this.regFactor) {
            this.loss = tf.regularizers.l1l2({
                l1: this.regFactor.l1,
                l2: this.regFactor.l2
            });
        }
        else if (this.loss === TypeLoss.L2 && this.regFactor) {
            this.loss = tf.regularizers.l2({
                l2: this.regFactor.l2
            });
        }
        else {
            this.loss = tf.regularizers.l2();
        }
        // Random Engine
        if (Number.isInteger(this.randomState)) {
            this.randomEngine = Random.engines.mt19937().seed(this.randomState);
        }
        else {
            this.randomEngine = Random.engines.mt19937().autoSeed();
        }
    }
    /**
     * Train the base SGD
     * @param X - Matrix of data
     * @param y - Matrix of targets
     */
    fit(X = null, y = null) {
        validateFitInputs(X, y);
        // holds all the preprocessed X values
        // Clone according to the clone flag
        const clonedX = this.clone ? cloneDeep(X) : X;
        const clonedY = this.clone ? cloneDeep(y) : y;
        this.sgd(clonedX, clonedY);
    }
    /**
     * Save the model's checkpoint
     */
    toJSON() {
        return {
            learning_rate: this.learningRate,
            epochs: this.epochs,
            weights: [...this.weights.dataSync()],
            random_state: this.randomState
        };
    }
    /**
     * Restore the model from a checkpoint
     * @param learning_rate - Training learning rate
     * @param epochs - Number of model's training epochs
     * @param weights - Model's training state
     * @param random_state - Static random state for the model initialization
     */
    fromJSON({ learning_rate = 0.0001, epochs = 10000, weights = [], random_state = null } = {
        learning_rate: 0.0001,
        epochs: 10000,
        weights: [],
        random_state: null
    }) {
        this.learningRate = learning_rate;
        this.epochs = epochs;
        this.weights = tf.tensor(weights);
        this.randomState = random_state;
    }
    /**
     * Predictions according to the passed in test set
     * @param X - Matrix of data
     */
    predict(X = null) {
        validateMatrix2D(X);
        // Adding bias
        const biasX = this.addBias(X);
        const tensorX = tf.tensor(biasX);
        const yPred = tensorX.dot(this.weights);
        return [...yPred.dataSync()];
    }
    /**
     * Initialize weights based on the number of features
     *
     * @example
     * initializeWeights(3);
     * // this.w = [-0.213981293, 0.12938219, 0.34875439]
     *
     * @param nFeatures
     */
    initializeWeights(nFeatures) {
        const limit = 1 / Math.sqrt(nFeatures);
        const distribution = Random.real(-limit, limit);
        const getRand = () => distribution(this.randomEngine);
        this.weights = tf.tensor1d(range(0, nFeatures).map(() => getRand()));
    }
    /**
     * Adding bias to a given array
     *
     * @example
     * addBias([[1, 2], [3, 4]], 1);
     * // [[1, 1, 2], [1, 3, 4]]
     *
     * @param X
     * @param bias
     */
    addBias(X, bias = 1) {
        // TODO: Is there a TF way to achieve it?
        return X.reduce((sum, cur) => {
            sum.push([bias].concat(cur));
            return sum;
        }, []);
    }
    /**
     * SGD based on linear model to calculate coefficient
     * @param X - training data
     * @param y - target data
     */
    sgd(X, y) {
        const tensorX = tf.tensor2d(this.addBias(X));
        this.initializeWeights(tensorX.shape[1]);
        const tensorY = tf.tensor1d(y);
        const tensorLR = tf.tensor(this.learningRate);
        for (let e = 0; e < this.epochs; e++) {
            const yPred = tensorX.dot(this.weights);
            const gradW = tensorY
                .sub(yPred)
                .neg()
                .dot(tensorX)
                .add(this.loss.apply(this.weights));
            this.weights = this.weights.sub(tensorLR.mul(gradW));
        }
    }
}
/**
 * Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
 *
 * This estimator implements regularized linear models with
 * stochastic gradient descent (SGD) learning: the gradient of
 * the loss is estimated each sample at a time and the model is
 * updated along the way with a decreasing strength schedule
 * (aka learning rate). SGD allows minibatch (online/out-of-core)
 * learning, see the partial_fit method. For best results using
 * the default learning rate schedule, the data should have zero mean
 * and unit variance.
 *
 * @example
 * import { SGDClassifier } from 'machinelearn/linear_model';
 * const clf = new SGDClassifier();
 * const X = [[0., 0.], [1., 1.]];
 * const y = [0, 1];
 * clf.fit(X ,y);
 * clf.predict([[2., 2.]]); // result: [ 1 ]
 *
 */
export class SGDClassifier extends BaseSGD {
    /**
     * Predicted values with Math.round applied
     * @param X - Matrix of data
     */
    predict(X = null) {
        const results = super.predict(X);
        return results.map(x => Math.round(x));
    }
}
/**
 * Linear model fitted by minimizing a regularized empirical loss with SGD
 * SGD stands for Stochastic Gradient Descent: the gradient of the loss
 * is estimated each sample at a time and the model is updated along
 * the way with a decreasing strength schedule (aka learning rate).
 *
 * @example
 * import { SGDRegressor } from 'machinelearn/linear_model';
 * const reg = new SGDRegressor();
 * const X = [[0., 0.], [1., 1.]];
 * const y = [0, 1];
 * reg.fit(X, y);
 * reg.predict([[2., 2.]]); // result: [ 1.281828588248001 ]
 *
 */
export class SGDRegressor extends BaseSGD {
    /**
     * Predicted values
     * @param X - Matrix of data
     */
    predict(X = null) {
        return super.predict(X);
    }
}
//# sourceMappingURL=data:application/json;base64,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