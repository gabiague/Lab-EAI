"use strict";
var __extends = (this && this.__extends) || (function () {
    var extendStatics = Object.setPrototypeOf ||
        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||
        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };
    return function (d, b) {
        extendStatics(d, b);
        function __() { this.constructor = d; }
        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());
    };
})();
var __read = (this && this.__read) || function (o, n) {
    var m = typeof Symbol === "function" && o[Symbol.iterator];
    if (!m) return o;
    var i = m.call(o), r, ar = [], e;
    try {
        while ((n === void 0 || n-- > 0) && !(r = i.next()).done) ar.push(r.value);
    }
    catch (error) { e = { error: error }; }
    finally {
        try {
            if (r && !r.done && (m = i["return"])) m.call(i);
        }
        finally { if (e) throw e.error; }
    }
    return ar;
};
var __spread = (this && this.__spread) || function () {
    for (var ar = [], i = 0; i < arguments.length; i++) ar = ar.concat(__read(arguments[i]));
    return ar;
};
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (Object.hasOwnProperty.call(mod, k)) result[k] = mod[k];
    result["default"] = mod;
    return result;
};
Object.defineProperty(exports, "__esModule", { value: true });
var tf = __importStar(require("@tensorflow/tfjs"));
var lodash_1 = require("lodash");
var Random = __importStar(require("random-js"));
var ops_1 = require("../ops");
var TypeLoss;
(function (TypeLoss) {
    TypeLoss["L1"] = "L1";
    TypeLoss["L2"] = "L2";
    TypeLoss["L1L2"] = "L1L2";
})(TypeLoss = exports.TypeLoss || (exports.TypeLoss = {}));
/**
 * Ordinary base class for SGD classier or regressor
 * @ignore
 */
var BaseSGD = /** @class */ (function () {
    /**
     * @param preprocess - preprocess methodology can be either minmax or null. Default is minmax.
     * @param learning_rate - Used to limit the amount each coefficient is corrected each time it is updated.
     * @param epochs - Number of iterations.
     * @param clone - To clone the passed in dataset.
     */
    function BaseSGD(_a) {
        var _b = _a === void 0 ? {
            learning_rate: 0.0001,
            epochs: 10000,
            clone: true,
            random_state: null,
            loss: TypeLoss.L2,
            reg_factor: null
        } : _a, _c = _b.learning_rate, learning_rate = _c === void 0 ? 0.0001 : _c, _d = _b.epochs, epochs = _d === void 0 ? 10000 : _d, _e = _b.clone, clone = _e === void 0 ? true : _e, _f = _b.random_state, random_state = _f === void 0 ? null : _f, _g = _b.loss, loss = _g === void 0 ? TypeLoss.L2 : _g, _h = _b.reg_factor, reg_factor = _h === void 0 ? null : _h;
        this.clone = true;
        this.weights = null;
        this.learningRate = learning_rate;
        this.epochs = epochs;
        this.clone = clone;
        this.randomState = random_state;
        this.loss = loss;
        this.regFactor = reg_factor;
        // Setting a loss function according to the input option
        if (this.loss === TypeLoss.L1 && this.regFactor) {
            this.loss = tf.regularizers.l1({
                l1: this.regFactor.l1
            });
        }
        else if (this.loss === TypeLoss.L1L2 && this.regFactor) {
            this.loss = tf.regularizers.l1l2({
                l1: this.regFactor.l1,
                l2: this.regFactor.l2
            });
        }
        else if (this.loss === TypeLoss.L2 && this.regFactor) {
            this.loss = tf.regularizers.l2({
                l2: this.regFactor.l2
            });
        }
        else {
            this.loss = tf.regularizers.l2();
        }
        // Random Engine
        if (Number.isInteger(this.randomState)) {
            this.randomEngine = Random.engines.mt19937().seed(this.randomState);
        }
        else {
            this.randomEngine = Random.engines.mt19937().autoSeed();
        }
    }
    /**
     * Train the base SGD
     * @param X - Matrix of data
     * @param y - Matrix of targets
     */
    BaseSGD.prototype.fit = function (X, y) {
        if (X === void 0) { X = null; }
        if (y === void 0) { y = null; }
        ops_1.validateFitInputs(X, y);
        // holds all the preprocessed X values
        // Clone according to the clone flag
        var clonedX = this.clone ? lodash_1.cloneDeep(X) : X;
        var clonedY = this.clone ? lodash_1.cloneDeep(y) : y;
        this.sgd(clonedX, clonedY);
    };
    /**
     * Save the model's checkpoint
     */
    BaseSGD.prototype.toJSON = function () {
        return {
            learning_rate: this.learningRate,
            epochs: this.epochs,
            weights: __spread(this.weights.dataSync()),
            random_state: this.randomState
        };
    };
    /**
     * Restore the model from a checkpoint
     * @param learning_rate - Training learning rate
     * @param epochs - Number of model's training epochs
     * @param weights - Model's training state
     * @param random_state - Static random state for the model initialization
     */
    BaseSGD.prototype.fromJSON = function (_a) {
        var _b = _a === void 0 ? {
            learning_rate: 0.0001,
            epochs: 10000,
            weights: [],
            random_state: null
        } : _a, _c = _b.learning_rate, learning_rate = _c === void 0 ? 0.0001 : _c, _d = _b.epochs, epochs = _d === void 0 ? 10000 : _d, _e = _b.weights, weights = _e === void 0 ? [] : _e, _f = _b.random_state, random_state = _f === void 0 ? null : _f;
        this.learningRate = learning_rate;
        this.epochs = epochs;
        this.weights = tf.tensor(weights);
        this.randomState = random_state;
    };
    /**
     * Predictions according to the passed in test set
     * @param X - Matrix of data
     */
    BaseSGD.prototype.predict = function (X) {
        if (X === void 0) { X = null; }
        ops_1.validateMatrix2D(X);
        // Adding bias
        var biasX = this.addBias(X);
        var tensorX = tf.tensor(biasX);
        var yPred = tensorX.dot(this.weights);
        return __spread(yPred.dataSync());
    };
    /**
     * Initialize weights based on the number of features
     *
     * @example
     * initializeWeights(3);
     * // this.w = [-0.213981293, 0.12938219, 0.34875439]
     *
     * @param nFeatures
     */
    BaseSGD.prototype.initializeWeights = function (nFeatures) {
        var _this = this;
        var limit = 1 / Math.sqrt(nFeatures);
        var distribution = Random.real(-limit, limit);
        var getRand = function () { return distribution(_this.randomEngine); };
        this.weights = tf.tensor1d(lodash_1.range(0, nFeatures).map(function () { return getRand(); }));
    };
    /**
     * Adding bias to a given array
     *
     * @example
     * addBias([[1, 2], [3, 4]], 1);
     * // [[1, 1, 2], [1, 3, 4]]
     *
     * @param X
     * @param bias
     */
    BaseSGD.prototype.addBias = function (X, bias) {
        if (bias === void 0) { bias = 1; }
        // TODO: Is there a TF way to achieve it?
        return X.reduce(function (sum, cur) {
            sum.push([bias].concat(cur));
            return sum;
        }, []);
    };
    /**
     * SGD based on linear model to calculate coefficient
     * @param X - training data
     * @param y - target data
     */
    BaseSGD.prototype.sgd = function (X, y) {
        var tensorX = tf.tensor2d(this.addBias(X));
        this.initializeWeights(tensorX.shape[1]);
        var tensorY = tf.tensor1d(y);
        var tensorLR = tf.tensor(this.learningRate);
        for (var e = 0; e < this.epochs; e++) {
            var yPred = tensorX.dot(this.weights);
            var gradW = tensorY
                .sub(yPred)
                .neg()
                .dot(tensorX)
                .add(this.loss.apply(this.weights));
            this.weights = this.weights.sub(tensorLR.mul(gradW));
        }
    };
    return BaseSGD;
}());
exports.BaseSGD = BaseSGD;
/**
 * Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
 *
 * This estimator implements regularized linear models with
 * stochastic gradient descent (SGD) learning: the gradient of
 * the loss is estimated each sample at a time and the model is
 * updated along the way with a decreasing strength schedule
 * (aka learning rate). SGD allows minibatch (online/out-of-core)
 * learning, see the partial_fit method. For best results using
 * the default learning rate schedule, the data should have zero mean
 * and unit variance.
 *
 * @example
 * import { SGDClassifier } from 'machinelearn/linear_model';
 * const clf = new SGDClassifier();
 * const X = [[0., 0.], [1., 1.]];
 * const y = [0, 1];
 * clf.fit(X ,y);
 * clf.predict([[2., 2.]]); // result: [ 1 ]
 *
 */
var SGDClassifier = /** @class */ (function (_super) {
    __extends(SGDClassifier, _super);
    function SGDClassifier() {
        return _super !== null && _super.apply(this, arguments) || this;
    }
    /**
     * Predicted values with Math.round applied
     * @param X - Matrix of data
     */
    SGDClassifier.prototype.predict = function (X) {
        if (X === void 0) { X = null; }
        var results = _super.prototype.predict.call(this, X);
        return results.map(function (x) { return Math.round(x); });
    };
    return SGDClassifier;
}(BaseSGD));
exports.SGDClassifier = SGDClassifier;
/**
 * Linear model fitted by minimizing a regularized empirical loss with SGD
 * SGD stands for Stochastic Gradient Descent: the gradient of the loss
 * is estimated each sample at a time and the model is updated along
 * the way with a decreasing strength schedule (aka learning rate).
 *
 * @example
 * import { SGDRegressor } from 'machinelearn/linear_model';
 * const reg = new SGDRegressor();
 * const X = [[0., 0.], [1., 1.]];
 * const y = [0, 1];
 * reg.fit(X, y);
 * reg.predict([[2., 2.]]); // result: [ 1.281828588248001 ]
 *
 */
var SGDRegressor = /** @class */ (function (_super) {
    __extends(SGDRegressor, _super);
    function SGDRegressor() {
        return _super !== null && _super.apply(this, arguments) || this;
    }
    /**
     * Predicted values
     * @param X - Matrix of data
     */
    SGDRegressor.prototype.predict = function (X) {
        if (X === void 0) { X = null; }
        return _super.prototype.predict.call(this, X);
    };
    return SGDRegressor;
}(BaseSGD));
exports.SGDRegressor = SGDRegressor;
//# sourceMappingURL=data:application/json;base64,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