"use strict";
var __extends = (this && this.__extends) || (function () {
    var extendStatics = Object.setPrototypeOf ||
        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||
        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };
    return function (d, b) {
        extendStatics(d, b);
        function __() { this.constructor = d; }
        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());
    };
})();
Object.defineProperty(exports, "__esModule", { value: true });
var preprocessing_1 = require("../preprocessing");
var stochastic_gradient_1 = require("./stochastic_gradient");
/**
 * Linear least squares with l2 regularization.
 *
 * Mizimizes the objective function:
 *
 *
 * ||y - Xw||^2_2 + alpha * ||w||^2_2
 *
 *
 * This model solves a regression model where the loss function is the linear least squares function
 * and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
 * This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
 *
 * @example
 * import { Iris } from 'machinelearn/datasets';
 * import { Ridge } from 'machinelearn/linear_model';
 * (async function() {
 *   const irisData = new Iris();
 *   const {
 *     data,         // returns the iris data (X)
 *     targets,      // list of target values (y)
 *   } = await irisData.load(); // loads the data internally
 *
 *   const reg = new Ridge({ l2: 1 });
 *   reg.fit(data, target);
 *   reg.predict([[5.1,3.5,1.4,0.2]]);
 * })();
 *
 */
var Ridge = /** @class */ (function (_super) {
    __extends(Ridge, _super);
    /**
     * @param l2 - Regularizer factor
     * @param epochs - Number of epochs
     * @param learning_rate - learning rate
     */
    function Ridge(_a) {
        var _b = _a === void 0 ? {
            l2: null,
            epochs: 1000,
            learning_rate: 0.001
        } : _a, _c = _b.l2, l2 = _c === void 0 ? null : _c, _d = _b.epochs, epochs = _d === void 0 ? 1000 : _d, _e = _b.learning_rate, learning_rate = _e === void 0 ? 0.001 : _e;
        var _this = this;
        if (l2 === null) {
            throw TypeError('Ridge cannot be initiated with null l2');
        }
        _this = _super.call(this, {
            reg_factor: { l2: l2 },
            learning_rate: learning_rate,
            epochs: epochs,
            loss: stochastic_gradient_1.TypeLoss.L2.toString()
        }) || this;
        return _this;
    }
    return Ridge;
}(stochastic_gradient_1.SGDRegressor));
exports.Ridge = Ridge;
/**
 * Linear Model trained with L1 prior as regularizer (aka the Lasso)
 *
 * The optimization objective for Lasso is:
 *
 * (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 *
 * Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio value (no L2 penalty).
 *
 * @example
 * import { Iris } from 'machinelearn/datasets';
 * import { Lasso } from 'machinelearn/linear_model';
 * (async function() {
 *   const irisData = new Iris();
 *   const {
 *     data,         // returns the iris data (X)
 *     targets,      // list of target values (y)
 *   } = await irisData.load(); // loads the data internally
 *
 *   const reg = new Lasso({ degree: 2, l1: 1 });
 *   reg.fit(data, target);
 *   reg.predict([[5.1,3.5,1.4,0.2]]);
 * })();
 *
 */
var Lasso = /** @class */ (function (_super) {
    __extends(Lasso, _super);
    /**
     * @param degree - Polynomial feature extraction degree
     * @param l1 - Regularizer factor
     * @param epochs - Number of epochs
     * @param learning_rate - Learning rate
     */
    function Lasso(_a) {
        var _b = _a === void 0 ? {
            degree: null,
            l1: null,
            epochs: 1000,
            learning_rate: 0.001
        } : _a, _c = _b.degree, degree = _c === void 0 ? null : _c, l1 = _b.l1, _d = _b.epochs, epochs = _d === void 0 ? 1000 : _d, _e = _b.learning_rate, learning_rate = _e === void 0 ? 0.001 : _e;
        var _this = this;
        if (l1 === null) {
            throw TypeError('Lasso cannot be initiated with null l1');
        }
        if (degree === null) {
            throw TypeError('Lasso cannot be initiated with null degree');
        }
        _this = _super.call(this, {
            reg_factor: { l1: l1 },
            learning_rate: learning_rate,
            epochs: epochs,
            loss: stochastic_gradient_1.TypeLoss.L1.toString()
        }) || this;
        _this.degree = degree;
        return _this;
    }
    /**
     * Fit model with coordinate descent.
     * @param X - A matrix of samples
     * @param y - A vector of targets
     */
    Lasso.prototype.fit = function (X, y) {
        if (X === void 0) { X = null; }
        if (y === void 0) { y = null; }
        var polynomial = new preprocessing_1.PolynomialFeatures({ degree: this.degree });
        var newX = preprocessing_1.normalize(polynomial.transform(X));
        _super.prototype.fit.call(this, newX, y);
    };
    /**
     * Predict using the linear model
     * @param X - A matrix of test data
     */
    Lasso.prototype.predict = function (X) {
        if (X === void 0) { X = null; }
        var polynomial = new preprocessing_1.PolynomialFeatures({ degree: this.degree });
        var newX = preprocessing_1.normalize(polynomial.transform(X));
        return _super.prototype.predict.call(this, newX);
    };
    return Lasso;
}(stochastic_gradient_1.SGDRegressor));
exports.Lasso = Lasso;
//# sourceMappingURL=data:application/json;base64,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