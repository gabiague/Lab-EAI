declare module "lib/types/matrix.types" {
    /**
     * Matrix Type covers up to 4D array
     */
    export type TypeMatrix<T> = T[] | T[][] | T[][] | T[][][] | T[][][][] | T[][][][][] | T[][][][][][];
    /**
     * Typing for a 1D matrix
     */
    export type Type1DMatrix<T> = T[];
    /**
     * Typing for a 2D matrix
     */
    export type Type2DMatrix<T> = T[][];
    /**
     * Typing for a 3D matrix
     */
    export type Type3DMatrix<T> = T[][][];
    /**
     * Typeing for a 4D matrix
     */
    export type Type4DMatrix<T> = T[][][][];
}
declare module "lib/types/model.types" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/matrix.types";
    /**
     * Base typing for ModelState. The typing ensures that model state can only have string, number, number[], string[]
     * or ModelState itself for recursive object typing
     */
    export interface TypeModelState {
        [key: string]: any;
    }
    /**
     * Base type definition for all the models
     * @ignore
     */
    export abstract class IMlModel<T> {
        /**
         * Fit data to build model
         */
        abstract fit(X: Type2DMatrix<T | number>, y?: Type1DMatrix<T>): void;
        /**
         * Predict multiple rows. Each row has a feature data for a prediction
         */
        abstract predict(X: Type2DMatrix<T | number> | Type1DMatrix<T>): T[] | T[][];
        /**
         * Restores model from a checkpoint
         */
        abstract fromJSON(state: TypeModelState): void;
        /**
         * Returns a model checkpoint
         */
        abstract toJSON(): TypeModelState;
    }
}
declare module "lib/types/index" {
    import { Type1DMatrix, Type2DMatrix, Type3DMatrix, Type4DMatrix, TypeMatrix } from "lib/types/matrix.types";
    import { IMlModel, TypeModelState } from "lib/types/model.types";
    export { TypeMatrix, TypeModelState, Type1DMatrix, Type2DMatrix, Type3DMatrix, Type4DMatrix, IMlModel };
}
declare module "lib/ops/tensor_ops" {
    import { Type1DMatrix, Type2DMatrix, TypeMatrix } from "lib/types/index";
    /**
     * Infers shape of a tensor using TF
     *
     * @example
     * inferShape(1) // exception
     * inferShape(true) // exception
     * inferShape([1, 2]) // [2]
     * inferShape([[1, 2], [3, 4]]) // [2, 2]
     *
     * @param X
     * @ignore
     */
    export function inferShape(X: TypeMatrix<any>): number[];
    /**
     * Validates the input matrix's types with the targetted types.
     * Specified target types must be one of https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof#Description
     *
     * @example
     * validateMatrixType([['z', 'z']],['string']); // no errors
     * validateMatrixType([['z', 'z']],['test']); // error: Input matrix type of ["string"] does not match with the target types ["test"]
     *
     * @param X - The input matrix
     * @param targetTypes - Target matrix types
     * @ignore
     */
    export function validateMatrixType(X: TypeMatrix<any>, targetTypes: string[]): void;
    /**
     * Validate typical X and y train data and check they are 2D and 1D shaped respectively
     *
     * @example
     * validateTrainInputs([ [1, 2], [3, 4] ], [ 1, 2 ]) // No errors
     * validateTrainInputs([ [[1, 2], [3, 3]], [[1, 2], [3, 3]] ], [ 1, 2 ]) // Error: The matrix is not 1D shaped: [ [[1, 2], [3, 3]], [[1, 2], [3, 3]] ] of [2, 2, 2]
     *
     * @param X
     * @param y
     * @ignore
     */
    export function validateFitInputs(X: Type2DMatrix<any>, y: Type1DMatrix<any>): void;
    /**
     * Validate the matrix is 1D shaped by checking the shape's length is exactly  1
     * @param X
     * @ignore
     */
    export function validateMatrix1D(X: Type1DMatrix<any>): number[];
    /**
     * Validate the matrix is 2D shaped by checking the shape's length is exactly 2
     * @param X - An input array
     * @ignore
     */
    export function validateMatrix2D(X: Type2DMatrix<any>): number[];
    /**
     * Reshapes any size of array into a new shape.
     *
     * The code was borrowed from math.js (https://github.com/josdejong/mathjs/blob/5750a1845442946d236822505c607a522be23474/src/utils/array.js#L258),
     * which enables us to use a specific method from Math.js instead of installing an entire library.
     *
     * TF.js has implemented an efficient way to return raw values from its Tensor implementation that always returns a 1D array,
     * which is not ideal in situations where we need a return value with correct shapes.
     *
     * Please check out https://github.com/tensorflow/tfjs/issues/939 for more information
     *
     * @example
     * reshape([1, 2, 3, 4, 5, 6], [2, 3]); // [[1, 2, 3], [4, 5, 6]]
     * reshape([1, 2, 3, 4, 5, 6], [2, 3, 1]); // [[[1], [2], [3]], [[4], [5], [6]]]
     *
     * @param array - Target array
     * @param sizes - New array shape to resize into
     * @ignore
     */
    export function reshape(array: TypeMatrix<any>, sizes: number[]): TypeMatrix<any>;
}
declare module "lib/ops/index" {
    import { inferShape, reshape, validateFitInputs, validateMatrix1D, validateMatrix2D, validateMatrixType } from "lib/ops/tensor_ops";
    export { inferShape, validateFitInputs, validateMatrix1D, validateMatrix2D, validateMatrixType, reshape };
}
declare module "lib/utils/MathExtra" {
    const math: {
        covariance: (X: any, xMean: any, y: any, yMean: any) => number;
        euclideanDistance: (v1: number[], v2: number[]) => number;
        hstack: (X: any, y: any) => any[];
        isArrayOf: (arr: any, _type?: string) => boolean;
        inner: (a: any, b: any) => any;
        isMatrix: (matrix: any) => boolean;
        isMatrixOf: (matrix: any, _type?: string) => boolean;
        manhattanDistance: (v1: number[], v2: number[]) => number;
        range: (start: number, stop: number) => number[];
        subset: (X: any, rowsRange: number[], colsRange: number[], replacement?: any) => any[][];
        size: (X: any, axis?: number) => number;
        subtract: (X: any, y: any) => any;
        variance: (X: any, mean: any) => number;
    };
    export default math;
}
declare module "lib/cluster/k_means" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    export interface KMeansOptions {
        k: number;
        distance?: 'euclidean' | 'manhattan';
        maxIteration?: number;
        randomState?: number;
    }
    /**
     * K-Means clustering
     *
     * @example
     * import { KMeans } from 'machinelearn/cluster';
     *
     * const kmean = new KMeans({ k: 2 });
     * const clusters = kmean.fit([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]);
     *
     * const result = kmean.predict([[0, 0], [4, 4]]);
     * // results in: [0, 1]
     */
    export class KMeans implements IMlModel<number> {
        private assignment;
        private centroids;
        private clusters;
        private distance;
        private k;
        private randomState;
        private maxIteration;
        /**
         *
         * @param distance - Choice of distance method. Defaulting to euclidean
         * @param k - Number of clusters
         * @param maxIteration - Relative tolerance with regards to inertia to declare convergence
         * @param randomState - Random state value for sorting centroids during the getInitialCentroid phase
         */
        constructor({ distance, k, maxIteration, randomState }?: KMeansOptions);
        /**
         * Compute k-means clustering.
         * @param {any} X - array-like or sparse matrix of shape = [n_samples, n_features]
         * @returns {{centroids: number[]; clusters: number[]}}
         */
        fit(X?: Type2DMatrix<number>): void;
        /**
         * Predicts the cluster index with the given X
         * @param {any} X - array-like or sparse matrix of shape = [n_samples, n_features]
         * @returns {number[]}
         */
        predict(X?: Type2DMatrix<number>): number[];
        /**
         * Get the model details in JSON format
         * @returns {{k: number; clusters: number[]; centroids: number[]}}
         */
        toJSON(): {
            k: number;
            clusters: Type1DMatrix<number>;
            centroids: Type2DMatrix<number>;
        };
        /**
         * Restores the model from checkpoints
         * @param {number} k
         * @param {number[]} clusters
         * @param {number[]} centroids
         */
        fromJSON({ k, clusters, centroids }: {
            k: number;
            clusters: Type1DMatrix<number>;
            centroids: Type2DMatrix<number>;
        }): void;
        /**
         * Get initial centroids from X of k
         * @param {number[]} X
         * @param {number} k
         * @returns {number[]}
         */
        private getInitialCentroids;
        /**
         * Get closest centroids based on the passed in distance method
         * @param {number[]} data
         * @param {number[]} centroids
         * @param distance
         * @returns {number}
         */
        private getClosestCentroids;
    }
}
declare module "lib/cluster/index" {
    import { KMeans } from "lib/cluster/k_means";
    export { KMeans };
}
declare module "lib/preprocessing/label" {
    import { Type1DMatrix } from "lib/types/index";
    /**
     * Encode labels with value between 0 and n_classes-1.
     *
     * @example
     * import { LabelEncoder } from 'machinelearn/preprocessing';
     *
     * const labelEncoder = new LabelEncoder();
     * const labelX = ['amsterdam', 'paris', 'tokyo'];
     * labelEncoder.fit(labelX);
     * const transformX = ['tokyo', 'tokyo', 'paris'];
     * const result = labelEncoder.transform(transformX);
     * // [ 2, 2, 1 ]
     */
    export class LabelEncoder {
        private classes;
        /**
         * Fit label encoder
         * @param {any[]} X - Input data in array or matrix
         */
        fit(X?: Type1DMatrix<string>): void;
        /**
         * Transform labels to normalized encoding.
         *
         * Given classes of ['amsterdam', 'paris', 'tokyo']
         *
         * It transforms ["tokyo", "tokyo", "paris"]
         *
         * Into [2, 2, 1]
         * @param X - Input data to transform according to the fitted state
         */
        transform(X?: Type1DMatrix<string>): any[];
    }
}
declare module "lib/datasets/BaseDataset" {
    import 'isomorphic-fetch';
    /**
     * @ignore
     */
    export class BaseDataset {
        /**
         * fetch load from a multiple
         * @param sources - A list of URLs to fetch the data from
         * @param type - type of data; for example CSV or JSON
         * @param delimiter - specify the data delimiter, which will be used to split the row data
         * @param lastIsTarget - tell the underlying processor that the last index of the dataset is the target data
         * @param trainType - data type to enforce on the training dataset
         * @param targetType - target type to enforce on the target dataset
         * @private
         */
        protected fetchLoad(sources?: any[], { type, delimiter, lastIsTarget, trainType, targetType }?: {
            type?: string;
            delimiter?: string;
            lastIsTarget?: true;
            trainType?: string;
            targetType?: string;
        }): Promise<{
            data: any;
            targets: any;
            labels: any;
        }>;
        /**
         * Load data from the local data folder
         */
        protected fsLoad(type: string, { delimiter, lastIsTarget, trainType, targetType }?: {
            delimiter?: string;
            lastIsTarget?: true;
            trainType?: string;
            targetType?: string;
        }): Promise<{
            data: any;
            targets: any;
            labels: any;
        }>;
        /**
         * Processes CSV type dataset. Returns a training and testing data pair
         * @param data - a raw string data
         * @param delimiter - delimiter to split on
         * @param lastIsTarget - flag to indicate that the last element is the target data
         * @param trainType - training data type to enforce
         * @param targetType - target data type to enforce
         */
        private processCSV;
    }
}
declare module "lib/datasets/Boston" {
    /**
     * References:
     * - https://www.kaggle.com/c/boston-housing/data
     * - https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
     */
    import { BaseDataset } from "lib/datasets/BaseDataset";
    /**
     * This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.
     * It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston),
     * and has been used extensively throughout the literature to benchmark algorithms.
     * However, these comparisons were primarily done outside of Delve and are thus somewhat suspect.
     * The dataset is small in size with only 506 cases.
     *
     * The data was originally published by Harrison, D. and Rubinfeld, D.L.
     * `Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.
     *
     * @example
     * import { Boston } from "machinelearn/datasets";
     *
     * (async function() {
     *   const bostonData = new Boston();
     *   const {
     *     data,
     *     targets,
     *     labels,
     *   } = await bostonData.load();
     * });
     *
     */
    export class Boston extends BaseDataset {
        /**
         * Load the dataset
         */
        load(): Promise<{
            /**
             * Training data
             */
            data: any[][];
            /**
             * Target data
             */
            targets: any[];
            /**
             * Real labels
             */
            labels: string[];
        }>;
    }
}
declare module "lib/datasets/Iris" {
    import 'isomorphic-fetch';
    import { BaseDataset } from "lib/datasets/BaseDataset";
    /**
     * The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher
     * in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.
     *
     * It contains 50 samples with 3 classes of 'Setosa', 'versicolor' and 'virginica'
     *
     * @example
     * import { Iris } from 'machinelearn/datasets';
     *
     * (async function() {
     *   const irisData = new Iris();
     *   const {
     *     data,         // returns the iris data (X)
     *     targets,      // list of target values (y)
     *     labels,       // list of labels
     *     targetNames,  // list of short target labels
     *     description   // dataset description
     *   } = await irisData.load(); // loads the data internally
     * })();
     *
     */
    export class Iris extends BaseDataset {
        /**
         * Load datasets
         */
        load(): Promise<{
            /**
             * Training data
             */
            data: any[][];
            /**
             * Target data
             */
            targets: any[];
            /**
             * Real labels
             */
            labels: string[];
            /**
             * Short labels
             */
            targetNames: string[];
            /**
             * Dataset description
             */
            description: string;
        }>;
    }
}
declare module "lib/datasets/index" {
    import { Boston } from "lib/datasets/Boston";
    import { Iris } from "lib/datasets/Iris";
    export { Boston, Iris };
}
declare module "lib/decomposition/pca" {
    import { IMlModel, Type2DMatrix } from "lib/types/index";
    /**
     * Principal component analysis (PCA)
     *
     * Linear dimensionality reduction using Singular Value Decomposition of
     * the data to project it to a lower dimensional space.
     *
     * - It uses the LAPACK implementation of the full SVD
     * - or randomized a randomised truncated SVD by the method of
     * Halko et al. 2009, depending on the shape
     * of the input data and the number of components to extract. (Will be implemented)
     *
     * @example
     * import { PCA } from 'machinelearn/decomposition';
     *
     * const pca = new PCA();
     * const X = [[1, 2], [3, 4], [5, 6]];
     * pca.fit(X);
     * console.log(pca.components); // result: [ [ 0.7071067811865476, 0.7071067811865474 ], [ 0.7071067811865474, -0.7071067811865476 ] ]
     * console.log(pca.explained_variance); // result: [ [ -0.3535533905932736, 0 ], [ 0, 0.5 ], [ 0.35355339059327373, 0 ] ]
     */
    export class PCA implements IMlModel<number> {
        /**
         * Principal axes in feature space, representing the directions of
         * maximum variance in the data. The components are sorted by explained_variance_.
         */
        components: any;
        /**
         * The amount of variance explained by each of the selected components.
         *
         * Equal to n_components largest eigenvalues of the covariance matrix of X.
         */
        explained_variance: any;
        /**
         * Fit the model with X.
         * At the moment it does not take n_components into consideration
         * so it will only calculate Singular value decomposition
         * @param {any} X
         */
        fit(X: Type2DMatrix<number>): void;
        /**
         * Predict does nothing in PCA
         * @param X - A 2D matrix
         */
        predict(X?: Type2DMatrix<number>): number[][];
        /**
         * Saves the model's states
         */
        toJSON(): {
            components: number[][];
            explained_variance: number[][];
        };
        /**
         * Restores the model from given states
         * @param components - Principal axes in feature space, representing the directions of maximum variance in the data.
         * @param explained_variance - The amount of variance explained by each of the selected components.
         */
        fromJSON({ components, explained_variance }?: {
            components: Type2DMatrix<number>;
            explained_variance: Type2DMatrix<number>;
        }): void;
    }
}
declare module "lib/decomposition/index" {
    import { PCA } from "lib/decomposition/pca";
    export { PCA };
}
declare module "lib/tree/tree" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Question used by decision tree algorithm to determine whether to split branch or not
     * @ignore
     */
    export class Question {
        private features;
        private column;
        private value;
        constructor(features: any, column: any, value: any);
        match(example: any): boolean;
        toString(): string;
    }
    /**
     * According to the given targets array, count occurrences into an object.
     * @param {any[]} targets - list of class: count
     * @returns {}
     * @ignore
     */
    export function classCounts(targets: any[]): {};
    /**
     * A leaf node that classifies data.
     * @ignore
     */
    export class Leaf {
        prediction: any;
        constructor(y: any);
    }
    /**
     * It holds a reference to the question, and to the two children nodes
     * @ignore
     */
    export class DecisionNode {
        question: any;
        trueBranch: any;
        falseBranch: any;
        constructor(question: any, trueBranch: any, falseBranch: any);
    }
    export interface Options {
        featureLabels?: null | any[];
        verbose?: boolean;
    }
    /**
     * A decision tree classifier.
     *
     * @example
     * import { DecisionTreeClassifier } from 'machinelearn/tree';
     * const features = ['color', 'diameter', 'label'];
     * const decision = new DecisionTreeClassifier({ featureLabels: features });
     *
     * const X = [['Green', 3], ['Yellow', 3], ['Red', 1], ['Red', 1], ['Yellow', 3]];
     * const y = ['Apple', 'Apple', 'Grape', 'Grape', 'Lemon'];
     * decision.fit({ X, y });
     * decision.printTree(); // try it out yourself! =)
     *
     * decision.predict({ X: [['Green', 3]] }); // [ 'Apple' ]
     * decision.predict({ X }); // [ [ 'Apple' ], [ 'Apple', 'Lemon' ], [ 'Grape', 'Grape' ], [ 'Grape', 'Grape' ], [ 'Apple', 'Lemon' ] ]
     *
     * @example
     * import { DecisionTreeClassifier } from 'machinelearn/tree';
     * const decision = new DecisionTreeClassifier({ featureLabels: null });
     *
     * const X = [[0, 0], [1, 1]];
     * const Y = [0, 1];
     * decision.fit({ X, y });
     * decision2.predict({ row: [[2, 2]] }); // [ 1 ]
     */
    export class DecisionTreeClassifier implements IMlModel<string | boolean | number> {
        private featureLabels;
        private tree;
        private verbose;
        private randomState;
        private randomEngine;
        /**
         *
         * @param featureLabels - Literal names for each feature to be used while printing the tree out as a string
         * @param verbose - Logs the progress of the tree construction as console.info
         * @param random_state - A seed value for the random engine
         */
        constructor({ featureLabels, verbose, random_state }?: {
            featureLabels?: any[];
            verbose?: boolean;
            random_state?: number;
        });
        /**
         * Fit date, which builds a tree
         * @param {any} X - 2D Matrix of training
         * @param {any} y - 1D Vector of target
         * @returns {Leaf | DecisionNode}
         */
        fit(X?: Type2DMatrix<string | number | boolean>, y?: Type1DMatrix<string | number | boolean>): void;
        /**
         * Predict multiple rows
         *
         * @param X - 2D Matrix of testing data
         */
        predict(X?: Type2DMatrix<string | boolean | number>): any[];
        /**
         * Returns the model checkpoint
         * @returns {{featureLabels: string[]; tree: any; verbose: boolean}}
         */
        toJSON(): {
            /**
             * Literal names for each feature to be used while printing the tree out as a string
             */
            featureLabels: string[];
            /**
             * The model's state
             */
            tree: any;
            /**
             * Logs the progress of the tree construction as console.info
             */
            verbose: boolean;
            /**
             * A seed value for the random engine
             */
            random_state: number;
        };
        /**
         * Restores the model from a checkpoint
         * @param {string[]} featureLabels - Literal names for each feature to be used while printing the tree out as a string
         * @param {any} tree - The model's state
         * @param {boolean} verbose - Logs the progress of the tree construction as console.info
         * @param {number} random_state - A seed value for the random engine
         */
        fromJSON({ featureLabels, tree, verbose, random_state }: {
            featureLabels: string[];
            tree: any;
            verbose: boolean;
            random_state: number;
        }): void;
        /**
         * Recursively print the tree into console
         * @param {string} spacing - Spacing used when printing the tree into the terminal
         */
        printTree(spacing?: string): void;
        /**
         * Partition X and y into true and false branches
         * @param X
         * @param y
         * @param {Question} question
         * @returns {{trueX: Array<any>; trueY: Array<any>; falseX: Array<any>; falseY: Array<any>}}
         */
        private partition;
        /**
         * Calculate the gini impurity of rows
         * Checkout: https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
         * @param targets
         * @returns {number}
         */
        private gini;
        /**
         * Information Gain.
         *
         * The uncertainty of the starting node, minus the weighted impurity of
         * two child nodes.
         * @param left
         * @param right
         * @param uncertainty
         * @returns {number}
         */
        private infoGain;
        /**
         * Find the best split for the current X and y.
         * @param X
         * @param y
         * @returns {{bestGain: number; bestQuestion: any}}
         */
        private findBestSplit;
        /**
         * Interactively build tree until it reaches the terminal nodes
         * @param {any} X
         * @param {any} y
         * @returns {any}
         */
        private buildTree;
        /**
         * Internal predict method separated out for recursion purpose
         * @param {any} row
         * @param {any} node
         * @returns {any}
         * @private
         */
        private _predict;
        /**
         * Private method for printing tree; required for recursion
         * @param {any} node
         * @param {any} spacing
         */
        private _printTree;
    }
}
declare module "lib/tree/index" {
    import { DecisionTreeClassifier } from "lib/tree/tree";
    export { DecisionTreeClassifier };
}
declare module "lib/ensemble/forest" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Base RandomForest implementation used by both classifier and regressor
     * @ignore
     */
    export class BaseRandomForest implements IMlModel<number> {
        protected trees: any[];
        protected nEstimator: any;
        protected randomState: any;
        /**
         *
         * @param {number} nEstimator - Number of trees.
         * @param random_state - Random seed value for DecisionTrees
         */
        constructor({ nEstimator, random_state }?: {
            nEstimator?: number;
            random_state?: number;
        });
        /**
         * Build a forest of trees from the training set (X, y).
         * @param {Array} X - array-like or sparse matrix of shape = [n_samples, n_features]
         * @param {Array} y - array-like, shape = [n_samples] or [n_samples, n_outputs]
         * @returns void
         */
        fit(X?: Type2DMatrix<number>, y?: Type1DMatrix<number>): void;
        /**
         * Returning the current model's checkpoint
         * @returns {{trees: any[]}}
         */
        toJSON(): {
            /**
             * Decision trees
             */
            trees: any[];
        };
        /**
         * Restore the model from a checkpoint
         * @param {any[]} trees - Decision trees
         */
        fromJSON({ trees }: {
            trees: any[];
        }): void;
        /**
         * Internal predict function used by either RandomForestClassifier or Regressor
         * @param X
         * @private
         */
        predict(X?: Type2DMatrix<number>): number[][];
    }
    /**
     * Random forest classifier creates a set of decision trees from randomly selected subset of training set.
     * It then aggregates the votes from different decision trees to decide the final class of the test object.
     *
     * @example
     * import { RandomForestClassifier } from 'machinelearn/ensemble';
     *
     * const X = [[0, 0], [1, 1], [2, 1], [1, 5], [3, 2]];
     * const y = [0, 1, 2, 3, 7];
     *
     * const randomForest = new RandomForestClassifier();
     * randomForest.fit(X, y);
     *
     * // Results in a value such as [ '0', '2' ].
     * // Predictions will change as we have not set a seed value.
     */
    export class RandomForestClassifier extends BaseRandomForest {
        /**
         * Predict class for X.
         *
         * The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates.
         * That is, the predicted class is the one with highest mean probability estimate across the trees.
         * @param {Array} X - array-like or sparse matrix of shape = [n_samples]
         * @returns {string[]}
         */
        predict(X?: Type2DMatrix<number>): any[];
        /**
         * @hidden
         * Bagging prediction helper method
         * According to the predictions returns by the trees, it will select the
         * class with the maximum number (votes)
         * @param {Array<any>} predictions - List of initial predictions that may look like [ [1, 2], [1, 1] ... ]
         * @returns {string[]}
         */
        private votePredictions;
    }
}
declare module "lib/ensemble/index" {
    import { RandomForestClassifier } from "lib/ensemble/forest";
    export { RandomForestClassifier };
}
declare module "lib/utils/nlp" {
    /**
     * @ignore
     */
    export class WordTokenizer {
        /**
         * Tokenize a given text
         * e.g.
         * given: "deep-learning ian good fellow learning jason shin shin"
         * returns: [ 'deep', 'learning', 'ian', 'good', 'fellow', 'learning', 'jason', 'shin', 'shin' ]
         * @param text
         * @returns {string[]}
         */
        tokenize(text: any): string[];
    }
}
declare module "lib/feature_extraction/stop_words" {
    /**
     * @ignore
     * @type {Array<string>}
     */
    export const ENGLISH_STOP_WORDS: string[];
}
declare module "lib/feature_extraction/text" {
    import { Type1DMatrix } from "lib/types/index";
    /**
     * The CountVectorizer provides a simple way to both tokenize a collection
     * of text documents and build a vocabulary of known words, but also
     * to encode new documents using that vocabulary.
     *
     * @example
     * import { CountVectorizer } from 'machinelearn/feature_extraction';
     *
     * const corpus = ['deep learning ian good fellow learning jason shin shin', 'yoshua bengio'];
     * const vocabCounts = cv.fit_transform(corpus);
     * console.log(vocabCounts); // [ [ 0, 1, 1, 1, 1, 1, 2, 2, 0 ], [ 1, 0, 0, 0, 0, 0, 0, 0, 1 ] ]
     * console.log(cv.vocabulary); // { bengio: 0, deep: 1, fellow: 2, good: 3, ian: 4, jason: 5, learning: 6, shin: 7, yoshua: 8 }
     * console.log(cv.getFeatureNames()); // [ 'bengio', 'deep', 'fellow', 'good', 'ian', 'jason', 'learning', 'shin', 'yoshua' ]
     *
     * const newVocabCounts = cv.transform(['ian good fellow jason duuog']);
     * console.log(newVocabCounts); // [ [ 0, 0, 1, 1, 1, 1, 0, 0, 0 ] ]
     */
    export class CountVectorizer {
        vocabulary: object;
        /** @ignore */
        private internalVocabulary;
        /**
         * Learn a vocabulary dictionary of all tokens in the raw documents.
         * @param {string[]} doc - An array of strings
         * @returns {CountVectorizer}
         */
        fit(doc?: Type1DMatrix<string>): this;
        /**
         * fit transform applies
         * @param {string[]} doc - An array of strings
         * @returns {number[][]}
         */
        fit_transform(doc?: Type1DMatrix<string>): number[][];
        /**
         * Transform documents to document-term matrix.
         * Extract token counts out of raw text documents using the vocabulary
         * fitted with fit or the one provided to the constructor.
         * @param {string[]} doc - An array of strings
         * @returns {number[][]}
         */
        transform(doc?: Type1DMatrix<string>): number[][];
        /**
         * Array mapping from feature integer indices to feature name
         * @returns {Object}
         */
        getFeatureNames(): object;
        /**
         * Build a tokenizer/vectorizer
         * @returns {(x: string) => string[]}
         */
        private buildAnalyzer;
        /**
         * Calculates list of vocabularies in the entire document and come up with
         * vocab: index pairs
         * @param doc
         */
        private buildVocabulary;
        /**
         * @ignore
         * Counting number of vocab occurences in the current token of a sentence
         * ['yoshua', 'bengio', 'deep', 'learning'] = vocabulary
         * ['yohua', 'bengio'] => tokens
         * results in
         * [1, 1, 0, 0]
         * @param doc
         */
        private countVocab;
        /**
         * @ignore
         * preprocess a line of text by applying
         * 1) tokenization
         * 2) removing stopwords
         * @param text
         * @param {any} removeSW
         * @returns {any}
         */
        private preprocess;
    }
}
declare module "lib/feature_extraction/index" {
    import { CountVectorizer } from "lib/feature_extraction/text";
    export { CountVectorizer };
}
declare module "lib/utils/permutations" {
    /**
     * Generate n combinations with repeat values.
     * @param X - Matrix input
     * @param n - number of repeats
     * @ignore
     */
    export function combinationsWithReplacement(X: any, n?: number): number[][];
}
declare module "lib/preprocessing/data" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Augment dataset with an additional dummy feature.
     * This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.
     *
     * @example
     * import { add_dummy_feature } from 'machinelearn/preprocessing';
     * const dummy = add_dummy_feature([[0, 1, 2], [1, 0, 3]]);
     * console.log(dummy); // returns: [ [ 1, 0, 1, 2 ], [ 1, 1, 0, 3 ] ]
     *
     * @param X - A matrix of data
     * @param value - Value to use for the dummy feature.
     */
    export function add_dummy_feature(X?: Type2DMatrix<number>, value?: number): number[][];
    /**
     * Encode categorical integer features using a one-hot aka one-of-K scheme.
     *
     * The input to this transformer should be a matrix of integers, denoting the
     * values taken on by categorical (discrete) features. The output will be a sparse
     * matrix where each column corresponds to one possible value of one feature.
     * It is assumed that input features take on values in the range [0, n_values).
     *
     * This encoding is needed for feeding categorical data to many
     * scikit-learn estimators, notably linear models and SVMs with the standard kernels.
     *
     * Note: a one-hot encoding of y labels should use a LabelBinarizer instead.
     *
     * @example
     * const enc = new OneHotEncoder();
     * const planetList = [
     *  { planet: 'mars', isGasGiant: false, value: 10 },
     *  { planet: 'saturn', isGasGiant: true, value: 20 },
     *  { planet: 'jupiter', isGasGiant: true, value: 30 }
     * ];
     * const encodeInfo = enc.encode(planetList, {
     *  dataKeys: ['value', 'isGasGiant'],
     *  labelKeys: ['planet']
     * });
     * // encodeInfo.data -> [ [ -1, 0, 1, 0, 0 ], [ 0, 1, 0, 1, 0 ], [ 1, 1, 0, 0, 1 ] ]
     * const decodedInfo = enc.decode(encodeInfo.data, encodeInfo.decoders);
     * // gives you back the original value, which is `planetList`
     */
    export class OneHotEncoder {
        /**
         * encode data according to dataKeys and labelKeys
         *
         * @param data - list of records to encode
         * @param options
         */
        encode(data?: any, { 
        /**
         * Independent variables
         */
        dataKeys, 
        /**
         * Depdenent variables
         */
        labelKeys }?: {
            dataKeys: Type1DMatrix<string>;
            labelKeys: Type1DMatrix<string>;
        }): {
            /**
             * Encoded data
             */
            data: any[];
            /**
             * Decoder
             */
            decoders: any[];
        };
        /**
         * Decode the encoded data back into its original format
         */
        decode(encoded: any, decoders: any): any[];
        /**
         * Decode an encoded row back into its original format
         * @param row
         * @param decoders
         * @returns {Object}
         */
        private decodeRow;
        /**
         * Standardizing field
         * Example dataset:
         * [ { planet: 'mars', isGasGiant: false, value: 10 },
         * { planet: 'saturn', isGasGiant: true, value: 20 },
         * { planet: 'jupiter', isGasGiant: true, value: 30 } ]
         *
         * @param key: each key/feature such as planet, isGasGiant and value
         * @param data: the entire dataset
         * @returns {any}
         */
        private standardizeField;
        /**
         * Calculating the sample standard deviation (vs population stddev).
         * @param lst
         * @param {number} mean
         * @returns {number}
         */
        private calculateStd;
        /**
         * One hot encode a number value
         *
         * @param type
         * @param key
         * @param values
         * @returns {{encoded: any[]; decode: {type: any; mean: number; std: number; key: any}}}
         */
        private buildNumberOneHot;
        /**
         * One hot encode a boolean value
         *
         * Example usage:
         * boolEncoder.encode(true) => 1
         * boolEncoder.encode(false) => 0
         *
         * @param type
         * @param key
         * @param values
         * @returns {{encode}}
         */
        private buildBooleanOneHot;
        /**
         * One hot encode a string value
         *
         * Example for internal reference (unnecessary details for those just using this module)
         *
         * const encoder = buildOneHot(['RAIN', 'RAIN', 'SUN'])
         * // encoder == { encode: () => ... , lookupTable: ['RAIN', 'SUN'] }
         * encoder.encode('SUN')  // [0, 1]
         * encoder.encode('RAIN') // [1, 0]
         * encoder.encode('SUN')  // [1, 0]
         * // encoder.lookupTable can then be passed into this.decode to translate [0, 1] back into 'SUN'
         *
         * It's not ideal (ideally it would all just be done in-memory and we could return a "decode" closure,
         * but it needs to be serializable to plain old JSON.
         */
        private buildStringOneHot;
    }
    /**
     * Transforms features by scaling each feature to a given range.
     *
     * This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.
     *
     * The transformation is given by:
     *
     * ```
     * X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
     * X_scaled = X_std * (max - min) + min
     * ```
     *
     * where min, max = feature_range.
     *
     * This transformation is often used as an alternative to zero mean, unit variance scaling.
     *
     * @example
     * import { MinMaxScaler } from 'machinelearn/preprocessing';
     *
     * const minmaxScaler = new MinMaxScaler({ featureRange: [0, 1] });
     *
     * // Fitting an 1D matrix
     * minmaxScaler.fit([4, 5, 6]);
     * const result = minmaxScaler.transform([4, 5, 6]);
     * // result = [ 0, 0.5, 1 ]
     *
     * // Fitting a 2D matrix
     * const minmaxScaler2 = new MinMaxScaler({ featureRange: [0, 1] });
     * minmaxScaler2.fit([[1, 2, 3], [4, 5, 6]]);
     * const result2 = minmaxScaler2.transform([[1, 2, 3]]);
     * // result2 = [ [ 0, 0.2, 0.4000000000000001 ] ]
     *
     */
    export class MinMaxScaler {
        private featureRange;
        private dataMax;
        private dataMin;
        private featureMax;
        private featureMin;
        private dataRange;
        private scale;
        private baseMin;
        /**
         * @param featureRange - scaling range
         */
        constructor({ featureRange }?: {
            featureRange?: number[];
        });
        /**
         * Compute the minimum and maximum to be used for later scaling.
         * @param {number[]} X - Array or sparse-matrix data input
         */
        fit(X?: Type1DMatrix<number> | Type2DMatrix<number>): void;
        /**
         * Fit to data, then transform it.
         * @param X - Original input vector
         */
        fit_transform(X: Type1DMatrix<number> | Type2DMatrix<number>): number[] | number[][];
        /**
         * Scaling features of X according to feature_range.
         * @param X - Original input vector
         */
        transform(X?: Type1DMatrix<number> | Type2DMatrix<number>): number[] | number[][];
        /**
         * Undo the scaling of X according to feature_range.
         * @param {number[]} X - Scaled input vector
         */
        inverse_transform(X?: Type1DMatrix<number>): number[];
    }
    /**
     * Binarizer transform your data using a binary threshold.
     * All values above the threshold are marked 1 and all equal to or below are marked as 0.
     *
     * It can also be used as a pre-processing step for estimators that consider
     * boolean random variables (e.g. modelled using the Bernoulli distribution in
     * a Bayesian setting).
     *
     * @example
     * import { Binarizer } from 'machinelearn/preprocessing';
     *
     * const binX = [[1, -1, 2], [2, 0, 0], [0, 1, -1]];
     * const binarizer = new Binarizer({ threshold: 0 });
     * const result = binarizer.transform(binX);
     * // [ [ 1, 0, 1 ], [ 1, 0, 0 ], [ 0, 1, 0 ] ]
     */
    export class Binarizer {
        private threshold;
        private copy;
        /**
         *
         * @param {number} threshold - Feature values below or equal to this are replaced by 0, above it by 1.
         * @param {boolean} copy - Flag to clone the input value.
         */
        constructor({ copy, threshold }?: {
            copy?: boolean;
            threshold?: number;
        });
        /**
         * Currently fit does nothing
         * @param {any[]} X - Does nothing
         */
        fit(X?: Type2DMatrix<number>): void;
        /**
         * Transforms matrix into binarized form
         * X = [[ 1., -1.,  2.],
         *      [ 2.,  0.,  0.],
         *      [ 0.,  1., -1.]]
         * becomes
         * array([[ 1.,  0.,  1.],
         *    [ 1.,  0.,  0.],
         *    [ 0.,  1.,  0.]])
         * @param {any[]} X - The data to binarize.
         */
        transform(X?: Type2DMatrix<number>): any[];
    }
    /**
     * Generate polynomial and interaction features.
     *
     * Generate a new feature matrix consisting of all polynomial combinations of the features
     * with degree less than or equal to the specified degree. For example, if an input sample
     * is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].
     *
     * @example
     * import { PolynomialFeatures } from 'machinelearn/preprocessing';
     * const poly = new PolynomialFeatures();
     * const X = [[0, 1], [2, 3], [4, 5]];
     * poly.transform(X);
     * // Result:
     * // [ [ 1, 0, 1, 0, 0, 1 ],
     * // [ 1, 2, 3, 4, 6, 9 ],
     * // [ 1, 4, 5, 16, 20, 25 ] ]
     *
     */
    export class PolynomialFeatures {
        private degree;
        /**
         *
         * @param degree - The degree of the polynomial features. Default = 2.
         */
        constructor({ degree }?: {
            degree: number;
        });
        /**
         * Transforms the input data
         * @param X - a matrix
         */
        transform(X?: Type2DMatrix<number>): number[][];
        /**
         * Creates a combination of index according to nFeautres and degree
         * @param nFeatures
         * @param degree
         */
        private indexCombination;
    }
    /**
     * Data normalization is a process of scaling dataset based on Vector Space Model, and by default, it uses L2 normalization.
     * At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional
     * to the square of the  β values, while the L1 norm is proportional the absolute value of the values in  β .
     *
     * @example
     * import { normalize } from 'machinelearn/preprocessing';
     *
     * const result = normalize([
     *   [1, -1, 2],
     *   [2, 0, 0],
     *   [0, 1, -1],
     * ], { norm: 'l2' });
     * console.log(result);
     * // [ [ 0.4082482904638631, -0.4082482904638631, 0.8164965809277261 ],
     * // [ 1, 0, 0 ],
     * // [ 0, 0.7071067811865475, -0.7071067811865475 ] ]
     *
     * @param X - The data to normalize
     * @param norm - The norm to use to normalize each non zero sample; can be either 'l1' or 'l2'
     * @return number[][]
     */
    export function normalize(X?: Type2DMatrix<number>, { norm }?: {
        norm: string;
    }): number[][];
}
declare module "lib/preprocessing/Imputer" {
    import { Type2DMatrix } from "lib/types/index";
    /**
     * Imputation transformer for completing missing values.
     *
     * @example
     * import { Imputer } from 'preprocessing/Imputer';
     *
     * const testX = [[1, 2], [null, 3], [7, 6]];
     * const imp = new Imputer({ missingValues: null, axis: 0 });
     * imp.fit(testX);
     * const impResult = imp.fit_transform([[null, 2], [6, null], [7, 6]]);
     * // [ [ 4, 2 ], [ 6, 3.6666666666666665 ], [ 7, 6 ] ]
     */
    export class Imputer {
        private missingValues;
        private strategy;
        private axis;
        private copy;
        private means;
        /**
         *
         * @param {any} missingValues - Target missing value to impute
         * @param {any} strategy - Missing value replacement strategy
         * @param {any} axis - Direction to impute
         * @param {any} copy - To clone the input value
         */
        constructor({ missingValues, strategy, axis, copy }: {
            missingValues?: any;
            strategy?: string;
            axis?: number;
            copy?: boolean;
        });
        /**
         * Fit the imputer on X.
         * @param {any[]} X - Input data in array or sparse matrix format
         */
        fit(X?: Type2DMatrix<any>): void;
        /**
         * Fit to data, then transform it.
         * @param {any[]} X - Input data in array or sparse matrix format
         * @returns {any[]}
         */
        fit_transform(X?: Type2DMatrix<any>): any[];
        /**
         * Calculate array of numbers as array of mean values
         * Examples:
         * [ [ 1, 2 ], [ null, 3 ], [ 123, 3 ] ]
         * => [ 1.5, 3, 63 ]
         *
         * [ [ 1, 123 ], [ 2, 3, 3 ] ]
         * => [ 62, 2.6666666666666665 ]
         *
         * @param matrix
         * @param {string[]} steps
         */
        private calcArrayMean;
    }
}
declare module "lib/preprocessing/index" {
    import { add_dummy_feature, Binarizer, MinMaxScaler, normalize, OneHotEncoder, PolynomialFeatures } from "lib/preprocessing/data";
    import { Imputer } from "lib/preprocessing/Imputer";
    export { add_dummy_feature, Binarizer, MinMaxScaler, normalize, OneHotEncoder, PolynomialFeatures, Imputer };
}
declare module "lib/linear_model/stochastic_gradient" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    export enum TypeLoss {
        L1 = "L1",
        L2 = "L2",
        L1L2 = "L1L2"
    }
    /**
     * Type for L1L2 regularizer factors
     */
    export interface TypeRegFactor {
        l1?: number;
        l2?: number;
    }
    /**
     * Ordinary base class for SGD classier or regressor
     * @ignore
     */
    export class BaseSGD implements IMlModel<number> {
        protected learningRate: number;
        protected epochs: number;
        protected loss: any;
        protected regFactor: TypeRegFactor;
        private clone;
        private weights;
        private randomEngine;
        private randomState;
        /**
         * @param preprocess - preprocess methodology can be either minmax or null. Default is minmax.
         * @param learning_rate - Used to limit the amount each coefficient is corrected each time it is updated.
         * @param epochs - Number of iterations.
         * @param clone - To clone the passed in dataset.
         */
        constructor({ learning_rate, epochs, clone, random_state, loss, reg_factor }?: {
            learning_rate?: number;
            epochs?: number;
            clone?: boolean;
            random_state?: number;
            loss?: string;
            reg_factor?: TypeRegFactor;
        });
        /**
         * Train the base SGD
         * @param X - Matrix of data
         * @param y - Matrix of targets
         */
        fit(X?: Type2DMatrix<number>, y?: Type1DMatrix<number>): void;
        /**
         * Save the model's checkpoint
         */
        toJSON(): {
            /**
             * model learning rate
             */
            learning_rate: number;
            /**
             * model training epochs
             */
            epochs: number;
            /**
             * Model training weights
             */
            weights: number[];
            /**
             * Number used to set a static random state
             */
            random_state: number;
        };
        /**
         * Restore the model from a checkpoint
         * @param learning_rate - Training learning rate
         * @param epochs - Number of model's training epochs
         * @param weights - Model's training state
         * @param random_state - Static random state for the model initialization
         */
        fromJSON({ learning_rate, epochs, weights, random_state }?: {
            learning_rate: number;
            epochs: number;
            weights: number[];
            random_state: number;
        }): void;
        /**
         * Predictions according to the passed in test set
         * @param X - Matrix of data
         */
        predict(X?: Type2DMatrix<number>): number[];
        /**
         * Initialize weights based on the number of features
         *
         * @example
         * initializeWeights(3);
         * // this.w = [-0.213981293, 0.12938219, 0.34875439]
         *
         * @param nFeatures
         */
        private initializeWeights;
        /**
         * Adding bias to a given array
         *
         * @example
         * addBias([[1, 2], [3, 4]], 1);
         * // [[1, 1, 2], [1, 3, 4]]
         *
         * @param X
         * @param bias
         */
        private addBias;
        /**
         * SGD based on linear model to calculate coefficient
         * @param X - training data
         * @param y - target data
         */
        private sgd;
    }
    /**
     * Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
     *
     * This estimator implements regularized linear models with
     * stochastic gradient descent (SGD) learning: the gradient of
     * the loss is estimated each sample at a time and the model is
     * updated along the way with a decreasing strength schedule
     * (aka learning rate). SGD allows minibatch (online/out-of-core)
     * learning, see the partial_fit method. For best results using
     * the default learning rate schedule, the data should have zero mean
     * and unit variance.
     *
     * @example
     * import { SGDClassifier } from 'machinelearn/linear_model';
     * const clf = new SGDClassifier();
     * const X = [[0., 0.], [1., 1.]];
     * const y = [0, 1];
     * clf.fit(X ,y);
     * clf.predict([[2., 2.]]); // result: [ 1 ]
     *
     */
    export class SGDClassifier extends BaseSGD {
        /**
         * Predicted values with Math.round applied
         * @param X - Matrix of data
         */
        predict(X?: Type2DMatrix<number>): number[];
    }
    /**
     * Linear model fitted by minimizing a regularized empirical loss with SGD
     * SGD stands for Stochastic Gradient Descent: the gradient of the loss
     * is estimated each sample at a time and the model is updated along
     * the way with a decreasing strength schedule (aka learning rate).
     *
     * @example
     * import { SGDRegressor } from 'machinelearn/linear_model';
     * const reg = new SGDRegressor();
     * const X = [[0., 0.], [1., 1.]];
     * const y = [0, 1];
     * reg.fit(X, y);
     * reg.predict([[2., 2.]]); // result: [ 1.281828588248001 ]
     *
     */
    export class SGDRegressor extends BaseSGD {
        /**
         * Predicted values
         * @param X - Matrix of data
         */
        predict(X?: Type2DMatrix<number>): number[];
    }
}
declare module "lib/linear_model/coordinate_descent" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/index";
    import { SGDRegressor } from "lib/linear_model/stochastic_gradient";
    /**
     * Linear least squares with l2 regularization.
     *
     * Mizimizes the objective function:
     *
     *
     * ||y - Xw||^2_2 + alpha * ||w||^2_2
     *
     *
     * This model solves a regression model where the loss function is the linear least squares function
     * and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
     * This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
     *
     * @example
     * import { Iris } from 'machinelearn/datasets';
     * import { Ridge } from 'machinelearn/linear_model';
     * (async function() {
     *   const irisData = new Iris();
     *   const {
     *     data,         // returns the iris data (X)
     *     targets,      // list of target values (y)
     *   } = await irisData.load(); // loads the data internally
     *
     *   const reg = new Ridge({ l2: 1 });
     *   reg.fit(data, target);
     *   reg.predict([[5.1,3.5,1.4,0.2]]);
     * })();
     *
     */
    export class Ridge extends SGDRegressor {
        /**
         * @param l2 - Regularizer factor
         * @param epochs - Number of epochs
         * @param learning_rate - learning rate
         */
        constructor({ l2, epochs, learning_rate }?: {
            l2: number;
            epochs?: number;
            learning_rate?: number;
        });
    }
    /**
     * Linear Model trained with L1 prior as regularizer (aka the Lasso)
     *
     * The optimization objective for Lasso is:
     *
     * (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
     *
     * Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio value (no L2 penalty).
     *
     * @example
     * import { Iris } from 'machinelearn/datasets';
     * import { Lasso } from 'machinelearn/linear_model';
     * (async function() {
     *   const irisData = new Iris();
     *   const {
     *     data,         // returns the iris data (X)
     *     targets,      // list of target values (y)
     *   } = await irisData.load(); // loads the data internally
     *
     *   const reg = new Lasso({ degree: 2, l1: 1 });
     *   reg.fit(data, target);
     *   reg.predict([[5.1,3.5,1.4,0.2]]);
     * })();
     *
     */
    export class Lasso extends SGDRegressor {
        private degree;
        /**
         * @param degree - Polynomial feature extraction degree
         * @param l1 - Regularizer factor
         * @param epochs - Number of epochs
         * @param learning_rate - Learning rate
         */
        constructor({ degree, l1, epochs, learning_rate }?: {
            degree: number;
            l1: number;
            epochs?: number;
            learning_rate?: number;
        });
        /**
         * Fit model with coordinate descent.
         * @param X - A matrix of samples
         * @param y - A vector of targets
         */
        fit(X?: Type2DMatrix<number>, y?: Type1DMatrix<number>): void;
        /**
         * Predict using the linear model
         * @param X - A matrix of test data
         */
        predict(X?: Type2DMatrix<number>): number[];
    }
}
declare module "lib/linear_model/linear_regression" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Type of Linear Regression
     * Univariate = It can handle a 1 dimensional input
     * Multivariate = It can handle a 2 dimensional input
     * @ignore
     */
    export enum TypeLinearReg {
        UNIVARIATE = "UNIVARIATE",
        MULTIVARIATE = "MULTIVARIATE"
    }
    /**
     * Ordinary least squares Linear Regression.
     *
     * It supports both univariate and multivariate linear regressions.
     *
     * @example
     * import { LinearRegression } from './linear_regression';
     * const linearRegression = new LinearRegression();
     * const X = [1, 2, 4, 3, 5];
     * const y = [1, 3, 3, 2, 5];
     * linearRegression.fit(X, y);
     * lr.predict([1, 2]);
     * // [ 1.1999999999999995, 1.9999999999999996 ]
     *
     * const linearRegression2 = new LinearRegression();
     * const X2 = [[1, 1], [1, 2], [2, 2], [2, 3]];
     * const y2 = [1, 1, 2, 2];
     * linearRegression2.fit(X2, y2);
     * lr.predict([[1, 2]]);
     * // [1.0000001788139343]
     */
    export class LinearRegression {
        private weights;
        private type;
        /**
         * fit linear model
         * @param {any} X - training values
         * @param {any} y - target values
         */
        fit(X?: Type1DMatrix<number> | Type2DMatrix<number>, y?: Type1DMatrix<number> | Type2DMatrix<number>): void;
        /**
         * Predict using the linear model
         * @param {number} X - Values to predict.
         * @returns {number}
         */
        predict(X?: Type1DMatrix<number> | Type2DMatrix<number>): number[];
        /**
         * Get the model details in JSON format
         */
        toJSON(): {
            /**
             * Coefficients
             */
            weights: number[];
            /**
             * Type of the linear regression model
             */
            type: TypeLinearReg;
        };
        /**
         * Restore the model from a checkpoint
         */
        fromJSON({ 
        /**
         * Model's weights
         */
        weights, 
        /**
         * Type of linear regression, it can be either UNIVARIATE or MULTIVARIATE
         */
        type }: {
            weights: number[];
            type: TypeLinearReg;
        }): void;
        /**
         * Univariate prediction
         * y = b0 + b1 * X
         *
         * @param X
         */
        private univariatePredict;
        /**
         * Multivariate prediction
         * y = (b0 * X0) + (b1 * X1) + (b2 * X2) + ....
         *
         * @param X
         */
        private multivariatePredict;
        /**
         * Calculates univariate coefficients for linear regression
         * @param X - X values
         * @param y - y targets
         */
        private calculateUnivariateCoeff;
        /**
         * Calculate multivariate coefficients for linear regression
         * @param X
         * @param y
         */
        private calculateMultiVariateCoeff;
    }
}
declare module "lib/linear_model/index" {
    import { Lasso, Ridge } from "lib/linear_model/coordinate_descent";
    import { LinearRegression } from "lib/linear_model/linear_regression";
    import { SGDClassifier, SGDRegressor, TypeLoss } from "lib/linear_model/stochastic_gradient";
    export { LinearRegression, Ridge, Lasso, SGDClassifier, SGDRegressor, TypeLoss };
}
declare module "lib/utils/validation" {
    /**
     * Check below array conditions
     * - multiclass
     *    - e.g. [ [1, 2], [2, 3] ]
     *      Then it sets multiclass value to true
     * - isArray<boolean>
     *   If the given arr is an array then the value is true else false
     * @param arr
     * @returns {any}
     * @ignore
     */
    export function checkArray(arr: any[]): {
        readonly isArray: boolean;
        readonly multiclass: boolean;
    };
}
declare module "lib/metrics/classification" {
    import { Type1DMatrix } from "lib/types/index";
    /**
     * Validator for classification exceptions
     * @param y_true
     * @param y_pred
     * @param labels
     * @param options
     * @ignore
     */
    export const validateInitialInputs: (y_true: any, y_pred: any, labels: any, options?: {}) => void;
    /**
     * Accuracy classification score.
     *
     * In multilabel classification, this function computes subset accuracy:
     * the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.
     *
     * @example
     * import { accuracyScore } from 'machinelearn/metrics';
     *
     * const accResult = accuracyScore(
     *  [0, 1, 2, 3],
     *  [0, 2, 1, 3]
     * );
     *
     * // accuracy result: 0.5
     *
     * @param y_true - 1d array-like, or label indicator array / sparse matrix
     * @param y_pred - 1d array-like, or label indicator array / sparse matrix
     * @param normalize
     */
    export function accuracyScore(y_true?: Type1DMatrix<number | string>, y_pred?: Type1DMatrix<number | string>, { normalize }?: {
        normalize: boolean;
    }): number;
    /**
     * Zero-one classification loss.
     *
     * If normalize is `true`, return the fraction of misclassifications (float),
     * else it returns the number of misclassifications (int). The best performance is 0.
     *
     * @example
     * import { zeroOneLoss } from 'machinelearn/metrics';
     *
     * const loss_zero_one_result = zeroOneLoss(
     *   [1, 2, 3, 4],
     *   [2, 2, 3, 5]
     * );
     * console.log(loss_zero_one_result); // 0.5
     *
     * @param {any} y_true - Ground truth (correct) labels.
     * @param {any} y_pred - Predicted labels, as returned by a classifier.
     * @param {any} normalize
     * @returns {number}
     */
    export function zeroOneLoss(y_true?: any, y_pred?: any, { 
    /**
     * If False, return the number of misclassifications. Otherwise, return the fraction of misclassifications.
     */
    normalize }?: {
        normalize: boolean;
    }): number;
    /**
     * A confusion matrix is a technique for summarizing the performance of a classification algorithm.
     *
     * Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset.
     *
     * Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.
     *
     * @example
     * import { confusion_matrix } from 'machinelearn/metrics';
     *
     * const matrix1 = confusion_matrix([1, 2, 3], [1, 2, 3]);
     * console.log(matrix1); // [ [ 1, 0, 0 ], [ 0, 1, 0 ], [ 0, 0, 1 ] ]
     *
     * const matrix2 = confusion_matrix(
     *   ['cat', 'ant', 'cat', 'cat', 'ant', 'bird'],
     *   ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
     * );
     * console.log(matrix2); // [ [ 1, 2, 0 ], [ 2, 0, 0 ], [ 0, 1, 0 ] ]
     *
     * @param y_true - Ground truth (correct) target values.
     * @param y_pred - Estimated targets as returned by a classifier.
     * @param labels
     */
    export function confusion_matrix(y_true?: Type1DMatrix<string | number>, y_pred?: Type1DMatrix<string | number>, { 
    /**
     * List of labels to index the matrix. This may be used to reorder or
     * select a subset of labels. If none is given, those that appear
     * at least once in y_true or y_pred are used in sorted order.
     */
    labels }?: {
        labels?: any[];
    }): number[];
}
declare module "lib/metrics/regression" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Mean absolute error regression loss
     *
     * @example
     * import { mean_absolute_error } from 'machinelearn/metrics';
     * const y_true = [3, -0.5, 2, 7]
     * const y_pred = [2.5, 0.0, 2, 8]
     * mean_absolute_error(y_true, y_pred); // 0.5
     *
     * @param y_true - Ground truth (correct) target values.
     * @param y_pred - Estimated target values.
     * @param sample_weight - Sample weights.
     */
    export function mean_absolute_error(y_true?: Type1DMatrix<number> | Type2DMatrix<number>, y_pred?: Type1DMatrix<number> | Type2DMatrix<number>, { sample_weight }?: {
        sample_weight: Type1DMatrix<number>;
    }): number;
    /**
     * Mean squared error regression loss
     *
     * @example
     * import { mean_squared_error } from 'machinelearn/metrics';
     *
     * const y_true = [3, -0.5, 2, 7];
     * const y_pred = [2.5, 0.0, 2, 8];
     *
     * console.log(mean_squared_error(y_true, y_pred));
     * // result: 0.375
     *
     * const y_true1 = [[0.5, 1], [-1, 1], [7, -6]];
     * const y_pred1 = [[0, 2], [-1, 2], [8, -5]];
     *
     * console.log(mean_squared_error(y_true1, y_pred1));
     * // result: 0.7083333134651184
     *
     * @param y_true - Ground truth (correct) target values.
     * @param y_pred - Estimated target values.
     */
    export function mean_squared_error(y_true?: Type1DMatrix<number> | Type2DMatrix<number>, y_pred?: Type1DMatrix<number> | Type2DMatrix<number>, { 
    /**
     * Sample weights.
     */
    sample_weight }?: {
        sample_weight: number;
    }): number;
}
declare module "lib/metrics/index" {
    import { accuracyScore, confusion_matrix, zeroOneLoss } from "lib/metrics/classification";
    import { mean_absolute_error, mean_squared_error } from "lib/metrics/regression";
    export { accuracyScore, confusion_matrix, mean_absolute_error, mean_squared_error, zeroOneLoss };
}
declare module "lib/model_selection/_split" {
    import { Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * K-Folds cross-validator
     *
     * Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).
     *
     * Each fold is then used once as a validation while the k - 1 remaining folds form the training set.
     *
     * @example
     * import { KFold } from 'machinelearn/model_selection';
     *
     * const kFold = new KFold({ k: 5 });
     * const X1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1];
     * console.log(kFold.split(X1, X1));
     *
     * /* [ { trainIndex: [ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19 ],
     * *  testIndex: [ 0, 1, 2, 3 ] },
     * * { trainIndex: [ 0, 1, 2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19 ],
     * *  testIndex: [ 4, 5, 6, 7 ] },
     * * { trainIndex: [ 0, 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19 ],
     * *  testIndex: [ 8, 9, 10, 11 ] },
     * * { trainIndex: [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 19 ],
     * *  testIndex: [ 12, 13, 14, 15 ] },
     * * { trainIndex: [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 ],
     * *  testIndex: [ 16, 17, 18, 19 ] } ]
     *
     */
    export class KFold {
        private k;
        private shuffle;
        /**
         *
         * @param {any} k - Number of folds. Must be at least 2.
         * @param {any} shuffle - Whether to shuffle the data before splitting into batches.
         */
        constructor({ k, shuffle }: {
            k?: number;
            shuffle?: boolean;
        });
        /**
         *
         * @param X - Training data, where n_samples is the number of samples and n_features is the number of features.
         * @param y - The target variable for supervised learning problems.
         * @returns {any[]}
         */
        split(X?: Type1DMatrix<any>, y?: Type1DMatrix<any>): any[];
    }
    /**
     * Split arrays or matrices into random train and test subsets
     *
     * @example
     * import { train_test_split } from 'machinelearn/model_selection';
     *
     * const X = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]];
     * const y = [0, 1, 2, 3, 4];
     *
     * train_test_split(X, y, {
     *   test_size: 0.33,
     *   train_size: 0.67,
     *   random_state: 42
     * });
     *
     * /*
     * * { xTest: [ [ 0, 1 ], [ 8, 9 ] ],
     * *  xTrain: [ [ 4, 5 ], [ 6, 7 ], [ 2, 3 ] ],
     * *  yTest: [ 0, 4 ],
     * *  yTrain: [ 2, 3, 1 ] }
     *
     * @param {any} X - input data
     * @param {any} y - target data
     * @param {number} test_size - size of the returning test set
     * @param {number} train_size - size of the returning training set
     * @param {number} random_state - state used to shuffle data
     * @param {boolean} clone - to clone the original data
     * @returns {{xTest: any[]; xTrain: any[]; yTest: any[]; yTrain: any[]}}
     */
    export function train_test_split(X?: Type2DMatrix<any>, y?: Type1DMatrix<any>, { test_size, train_size, random_state, clone }?: {
        test_size?: number;
        train_size?: number;
        random_state?: number;
        clone?: boolean;
    }): {
        xTest: any[];
        xTrain: any[];
        yTest: any[];
        yTrain: any[];
    };
}
declare module "lib/model_selection/index" {
    import { KFold, train_test_split } from "lib/model_selection/_split";
    export { KFold, train_test_split };
}
declare module "lib/neighbors/KDTree" {
    /**
     * @ignore
     */
    export class Node {
        obj: any;
        left: any;
        right: any;
        parent: any;
        dimension: any;
        constructor(obj: any, dimension: any, parent: any);
    }
    /**
     * @ignore
     */
    export class KDTree {
        dimensions: any;
        root: any;
        private metric;
        constructor(points: any, metric: any);
        toJSON(): Node;
        nearest(point: any, maxNodes: any, maxDistance: any): any[];
    }
}
declare module "lib/neighbors/classification" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Classifier implementing the k-nearest neighbors vote.
     *
     * @example
     * const knn = new KNeighborsClassifier();
     * const X = [[0, 0, 0], [0, 1, 1], [1, 1, 0], [2, 2, 2], [1, 2, 2], [2, 1, 2]];
     * const y = [0, 0, 0, 1, 1, 1];
     * knn.fit(X ,y);
     * console.log(knn.predict([1, 2])); // predicts 1
     */
    export class KNeighborsClassifier<T extends number | string | boolean> implements IMlModel<T> {
        private type;
        private tree;
        private k;
        private classes;
        private distance;
        /**
         * @param {string} distance - Choice of distance function, should choose between euclidean | manhattan
         * @param {number} k - Number of neighbors to classify
         * @param {string} type - Type of algorithm to use, choose between kdtree(default) | balltree | simple
         */
        constructor({ distance, k, type }?: {
            distance: string;
            k: number;
            type: string;
        });
        /**
         * Train the classifier with input and output data
         * @param {any} X - Training data.
         * @param {any} y - Target data.
         */
        fit(X: Type2DMatrix<T>, y: Type1DMatrix<T>): void;
        /**
         * Return the model's state as a JSON object
         * @return {object} JSON KNN model.
         */
        toJSON(): {
            classes: any[];
            distance: any;
            k: number;
            tree: any;
            type: string;
        };
        /**
         * Restores the model from a JSON checkpoint
         * @param {any} classes
         * @param {any} distance
         * @param {any} k
         * @param {any} tree
         * @param {any} type
         */
        fromJSON({ classes, distance, k, tree, type }: {
            classes?: any;
            distance?: any;
            k?: any;
            tree?: any;
            type?: any;
        }): void;
        /**
         * Predict single value from a list of data
         * @param {Array} X - Prediction data.
         * @returns number
         */
        predict(X: Type2DMatrix<T> | Type1DMatrix<T>): any;
        /**
         * Runs a single prediction against an array based on kdTree or balltree or
         * simple algo
         * @param array
         * @returns {{}}
         */
        private getSinglePred;
        /**
         * Get the class with the max point
         * @param current
         * @returns {{}}
         * @ignore
         */
        private getTreeBasedPrediction;
    }
}
declare module "lib/neighbors/index" {
    import { KNeighborsClassifier } from "lib/neighbors/classification";
    export { KNeighborsClassifier };
}
declare module "lib/svm/classes" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    export type Type = 'C_SVC' | 'NU_SVC' | 'ONE_CLASS' | 'EPSILON_SVR' | 'NU_SVR';
    export type Kernel = 'LINEAR' | 'POLYNOMIAL' | 'RBF' | 'SIGMOID';
    /**
     * Options used by sub classes
     * Notice type is disabled as they are set statically from children classes
     */
    export interface SVMOptions {
        /**
         * Degree of polynomial, test for polynomial kernel
         */
        degree?: number;
        /**
         * Type of Kernel
         */
        kernel?: Kernel;
        /**
         * Gamma parameter of the RBF, Polynomial and Sigmoid kernels. Default value is 1/num_features
         */
        gamma?: number | null;
        /**
         * coef0 parameter for Polynomial and Sigmoid kernels
         */
        coef0?: number;
        /**
         * Cost parameter, for C SVC, Epsilon SVR and NU SVR
         */
        cost?: number;
        /**
         * For NU SVC and NU SVR
         */
        nu?: number;
        /**
         * For epsilon SVR
         */
        epsilon?: number;
        /**
         * Cache size in MB
         */
        cacheSize?: number;
        /**
         * Tolerance
         */
        tolerance?: number;
        /**
         * Use shrinking euristics (faster)
         */
        shrinking?: boolean;
        /**
         * weather to train SVC/SVR model for probability estimates,
         */
        probabilityEstimates?: boolean;
        /**
         * Set weight for each possible class
         */
        weight?: object | null;
        /**
         * Print info during training if false (aka verbose)
         */
        quiet?: boolean;
    }
    /**
     * BaseSVM class used by all parent SVM classes that are based on libsvm
     */
    export class BaseSVM implements IMlModel<number> {
        protected svm: any;
        protected type: Type;
        protected options: SVMOptions;
        constructor(options?: SVMOptions);
        /**
         * Fit the model according to the given training data.
         * @param {number[][]} X
         * @param {number[]} y
         * @returns {Promise<void>}
         */
        fit(X: Type2DMatrix<number>, y: Type1DMatrix<number>): Promise<void>;
        /**
         * Predict using the linear model
         * @param {number[][]} X
         * @returns {number[]}
         */
        predict(X: Type2DMatrix<number>): number[];
        /**
         * Predict the label of one sample.
         * @param {number[]} X
         * @returns {number}
         */
        predictOne(X: Type1DMatrix<number>): number[];
        /**
         * Saves the current SVM as a JSON object
         * @returns {{svm: any; type: Type; options: SVMOptions}}
         */
        toJSON(): {
            svm: any;
            type: Type;
            options: SVMOptions;
        };
        /**
         * Restores the model from a JSON checkpoint
         * @param {any} svm
         * @param {any} type
         * @param {any} options
         */
        fromJSON({ svm, type, options }: {
            svm?: any;
            type?: any;
            options?: any;
        }): void;
        /**
         * Load SVM object by resolving the default promise
         * @returns {Promise<any>}
         */
        private loadSVM;
        /**
         * Get Kernel name type using string Kernel name
         * @param SVM
         * @param {string} name
         * @returns {number}
         */
        private getKernel;
        /**
         * Get Kernel type using string type name
         * @param SVM
         * @param {string} name
         * @returns {number}
         */
        private getType;
        /**
         * Get a consolidated options including type and Kernel
         * @param SVM
         * @param {Options} options
         * @param {Type} type
         * @param {Kernel} kernel
         * @returns {Object}
         */
        private processOptions;
    }
    /**
     * C-Support Vector Classification.
     *
     * The implementation is based on libsvm. The fit time complexity is more than
     * quadratic with the number of samples which makes it hard to scale to dataset
     * with more than a couple of 10000 samples.
     *
     * The multiclass support is handled according to a one-vs-one scheme.
     *
     * For details on the precise mathematical formulation of the provided kernel
     * functions and how gamma, coef0 and degree affect each other, see the corresponding
     * section in the narrative documentation: Kernel functions.
     */
    export class SVC extends BaseSVM {
        constructor(options?: SVMOptions);
    }
    /**
     * Linear Support Vector Regression.
     *
     * Similar to SVR with parameter kernel=’linear’, but implemented in terms of
     * liblinear rather than libsvm, so it has more flexibility in the choice of
     * penalties and loss functions and should scale better to large numbers of samples.
     *
     * This class supports both dense and sparse input.
     */
    export class SVR extends BaseSVM {
        constructor(options?: SVMOptions);
    }
    /**
     * Unsupervised Outlier Detection.
     *
     * Estimate the support of a high-dimensional distribution.
     *
     * The implementation is based on libsvm.
     */
    export class OneClassSVM extends BaseSVM {
        constructor(options?: SVMOptions);
    }
    /**
     * Nu-Support Vector Classification.
     *
     * Similar to SVC but uses a parameter to control the number of support vectors.
     *
     * The implementation is based on libsvm.
     */
    export class NuSVC extends BaseSVM {
        constructor(options?: SVMOptions);
    }
    /**
     * Nu Support Vector Regression.
     *
     * Similar to NuSVC, for regression, uses a parameter nu to control the number
     * of support vectors. However, unlike NuSVC, where nu replaces C, here nu
     * replaces the parameter epsilon of epsilon-SVR.
     *
     * The implementation is based on libsvm.
     */
    export class NuSVR extends BaseSVM {
        constructor(options?: SVMOptions);
    }
}
declare module "lib/svm/index" {
    import { BaseSVM, NuSVC, NuSVR, OneClassSVM, SVC, SVR } from "lib/svm/classes";
    export { BaseSVM, NuSVC, NuSVR, OneClassSVM, SVC, SVR };
}
declare module "lib/index" {
    import * as cluster from "lib/cluster/index";
    import * as datasets from "lib/datasets/index";
    import * as decomposition from "lib/decomposition/index";
    import * as ensemble from "lib/ensemble/index";
    import * as feature_extraction from "lib/feature_extraction/index";
    import * as linear_model from "lib/linear_model/index";
    import * as metrics from "lib/metrics/index";
    import * as model_selection from "lib/model_selection/index";
    import * as neighbors from "lib/neighbors/index";
    import * as preprocessing from "lib/preprocessing/index";
    import * as svm from "lib/svm/index";
    import * as tree from "lib/tree/index";
    export { cluster, datasets, decomposition, ensemble, feature_extraction, linear_model, metrics, model_selection, neighbors, preprocessing, svm, tree, };
}
declare module "lib/cluster/index.repl" { }
declare module "lib/datasets/index.repl" { }
declare module "lib/decomposition/index.repl" { }
declare module "lib/ensemble/index.repl" { }
declare module "lib/feature_extraction/index.repl" { }
declare module "lib/linear_model/index.repl" { }
declare module "lib/metrics/index.repl" { }
declare module "lib/model_selection/index.repl" { }
declare module "lib/naive_bayes/gaussian" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * The Naive is an intuitive method that uses probabilistic of each attribute
     * being in each class to make a prediction. It uses Gaussian function to estimate
     * probability of a given class.
     *
     * @example
     * import { GaussianNB } from 'machinelearn/naive_bayes';
     *
     * const nb = new GaussianNB();
     * const X = [[1, 20], [2, 21], [3, 22], [4, 22]];
     * const y = [1, 0, 1, 0];
     * nb.fit({ X, y });
     * nb.predict({ X: [[1, 20]] }); // returns [ 1 ]
     *
     */
    export class GaussianNB<T extends number | string = number> implements IMlModel<T> {
        private classCategories;
        private mean;
        private variance;
        /**
         * @param X - array-like or sparse matrix of shape = [n_samples, n_features]
         * @param y - array-like, shape = [n_samples] or [n_samples, n_outputs]
         */
        fit(X?: Type2DMatrix<number>, y?: Type1DMatrix<T>): void;
        /**
         * @param X - array-like, shape = [n_samples, n_features]
         */
        predict(X?: Type2DMatrix<number>): T[];
        /**
         * Restore the model from saved states
         * @param modelState
         */
        fromJSON({ classCategories, mean, variance }: {
            /**
             * List of class categories
             */
            classCategories: T[];
            /**
             * Mean of each feature per class
             */
            mean: Type2DMatrix<number>;
            /**
             * Variance of each feature per class
             */
            variance: Type2DMatrix<number>;
        }): void;
        /**
         * Save the model's states
         */
        toJSON(): {
            /**
             * List of class categories
             */
            classCategories: T[];
            /**
             * Mean of each feature per class
             */
            mean: Type2DMatrix<number>;
            /**
             * Variance of each feature per class
             */
            variance: Type2DMatrix<number>;
        };
        /**
         * Make a single prediction
         *
         * @param  {ReadonlyArray<number>} X- values to predict in Matrix format
         * @returns T
         */
        private singlePredict;
        /**
         * Summarise the dataset per class using "probability density function"
         *
         * @param  {Type2DMatrix<number>} X
         * @param  {ReadonlyArray<T>} y
         * @returns InterfaceFitModel
         */
        private fitModel;
    }
}
declare module "lib/naive_bayes/multinomial" {
    import { IMlModel, Type1DMatrix, Type2DMatrix } from "lib/types/index";
    /**
     * Multinomial naive bayes machine learning algorithm
     *
     * The Naive is an intuitive method that uses probabilistic of each attribute
     * being in each class to make a prediction. It uses multinomial function to estimate
     * probability of a given class.
     *
     * @example
     * import { MultinomialNB } from 'machinelearn/naive_bayes';
     *
     * const nb = new MultinomialNB();
     * const X = [[1, 20], [2, 21], [3, 22], [4, 22]];
     * const y = [1, 0, 1, 0];
     * nb.fit({ X, y });
     * nb.predict({ X: [[1, 20]] }); // returns [ 1 ]
     *
     */
    export class MultinomialNB<T extends number | string = number> implements IMlModel<T> {
        /**
         * List of classes
         * @example
         * Given [1, 0, 1, 0, 2, 2, 2], categories are
         * [0, 1, 2]
         */
        private classCategories;
        /**
         * Multinomial distribution values. It is always two dimensional values.
         */
        private multinomialDist;
        private priorProbability;
        private alpha;
        /**
         * Fit date to build Gaussian Distribution summary
         *
         * @param  {Type2DMatrix<number>} X - training values
         * @param  {ReadonlyArray<T>} y - target values
         * @returns void
         */
        fit(X?: Type2DMatrix<number>, y?: Type1DMatrix<T>): void;
        /**
         * Predict multiple rows
         *
         * @param  {Type2DMatrix<number>} X - values to predict in Matrix format
         * @returns T
         */
        predict(X?: Type2DMatrix<number>): T[];
        /**
         * Returns a model checkpoint
         *
         * @returns InterfaceFitModelAsArray
         */
        toJSON(): {
            /**
             * List of class categories
             */
            classCategories: T[];
            /**
             * Multinomial distribution values over classes
             */
            multinomialDist: Type2DMatrix<number>;
            /**
             * Learned prior class probabilities
             */
            priorProbability: Type1DMatrix<number>;
        };
        /**
         * Restore the model from states
         * @param multinomialDist - Multinomial distribution values over classes
         * @param priorProbability - Learned prior class probabilities
         * @param classCategories - List of unique class categories
         */
        fromJSON({ multinomialDist, priorProbability, classCategories }?: {
            multinomialDist: Type2DMatrix<number>;
            priorProbability: Type1DMatrix<number>;
            classCategories: Type1DMatrix<T>;
        }): void;
        /**
         * Make a prediction
         *
         * @param  {ReadonlyArray<number>} predictRow
         * @returns T
         */
        private singlePredict;
        /**
         * Summarise the dataset per class
         *
         * @param  {Type2DMatrix<number>} X - input distribution
         * @param  {ReadonlyArray<T>} y - classes to train
         */
        private fitModel;
    }
}
declare module "lib/naive_bayes/index" {
    import { GaussianNB } from "lib/naive_bayes/gaussian";
    import { MultinomialNB } from "lib/naive_bayes/multinomial";
    export { GaussianNB, MultinomialNB };
}
declare module "lib/naive_bayes/index.repl" { }
declare module "lib/neighbors/index.repl" { }
declare module "lib/ops/index.repl" { }
declare module "lib/preprocessing/index.repl" { }
declare module "lib/svm/index.repl" { }
declare module "lib/tree/index.repl" { }
